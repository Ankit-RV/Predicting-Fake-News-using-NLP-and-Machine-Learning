# -*- coding: utf-8 -*-
"""Predicting Fake News using NLP and Machine Learning .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12iUPnkW7mEjgs7Up-biJwJrVJjR_PErI
"""

!pip install contractions

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from matplotlib import rcParams
plt.rcParams['figure.figsize'] = [10,10]
import seaborn as sns
sns.set_theme(style="darkgrid")
from wordcloud import WordCloud

import nltk
from nltk import sent_tokenize
''' Required for Google Colab'''
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
''' Required for Google Colab'''
from nltk.corpus import stopwords
stopwords = nltk.corpus.stopwords.words('english')
from nltk.tokenize import word_tokenize

import contractions
import re
import itertools
import datetime
import time
from collections import Counter
import string

import warnings
warnings.filterwarnings('ignore')

"""I started with uploading the dataset on Google Colab.


"""

# Upload files

from google.colab import files
upload = files.upload()

from google.colab import files
upload = files.upload()

"""Next, I read the DataFrame and checked the null values in it.

There are 7 null values in the text articles, 122 in title and 503 in author out of a total of 20800 rows, I decided to drop the rows. For the test data, I filled them up with a blank.
"""

train_df = pd.read_csv('train.csv', header=0)
test_df = pd.read_csv('test.csv', header=0)

train_df.head()

train_df.shape

train_df.info()

sns.countplot(x='label', data=train_df, palette='Set3')
plt.title("Number of Fake and Genuine News before dropping missing values")
plt.show()

"""The labels look almost equally distributed."""

test_df.head()

test_df.shape

test_df.info()

train_df.isna().sum()

test_df.isna().sum()

"""**Drop all instances which have atleast one column missing**"""

train_df.dropna(axis=0, how='any',inplace=True)

test_df=test_df.fillna(' ')

train_df.shape, test_df.shape

train_df.info()

test_df.info()

sns.countplot(x='label', data=train_df, palette='Set3')
plt.title("Number of Fake and Genuine News after dropping missing values")
plt.show()

train_df = pd.read_csv('train.csv', header=0)
test_df = pd.read_csv('test.csv', header=0)
print(train_df.isna().sum())
print(test_df.isna().sum())
train_df.dropna(axis=0, how='any',inplace=True)
test_df = test_df.fillna(' ')

"""Additionally, I also check the distribution of ‘Fake’ and ‘Genuine’ news in the dataset. Usually, I set the rcParams for all plots on the notebook while importing matplotlib."""

import matplotlib.pyplot as plt
from matplotlib import rcParams
plt.rcParams['figure.figsize'] = [5, 5]
import seaborn as sns
sns.set_theme(style="darkgrid")

sns.countplot(x='label', data=train_df, palette='Set3')
plt.show()

"""The ratio is disturbed from being 1:1 to 4:5 for genuine to fake news.

**Check length of Text**
"""

train_df['raw_text_length'] = train_df['text'].apply(lambda x: len(x))

train_df.head()

sns.boxplot(y='raw_text_length', data=train_df, palette="Set3")
plt.show()

sns.boxplot(y='raw_text_length', x='label', data=train_df, palette="Set3")
plt.title("Boxplot of the length of characters in Fake and Genuine Articles")
plt.show()

train_df['raw_text_length'].describe()

train_df[train_df['raw_text_length']==1]

train_df['text'] = train_df['text'].str.strip()

train_df['raw_text_length'] = train_df['text'].apply(lambda x: len(x))

train_df['raw_text_length'].describe()

len(train_df[train_df['raw_text_length']==0])

"""It is seen that the median length is lower for fake articles but it also has loads of outliers. Both have zero length.

It is seen that they start from 0 which is concerning. It actually starts from 1 when I used .describe() to see the numbers. So I took a look at these texts and found that they are blank. The obvious answer to this is strip and drop length zero. I checked the total number of zero-length texts is 74.

#Start Over

since ~ 600 rows I am discarding. I would rather replace the authors as blank and keep the article texts
"""

train_df = pd.read_csv('train.csv', header=0)
train_df = train_df.fillna(' ')
train_df['text'] = train_df['text'].str.strip()
train_df['raw_text_length'] = train_df['text'].apply(lambda x: len(x))
print(len(train_df[train_df['raw_text_length']==0]))

train_df.isna().sum()

train_df = train_df[train_df['raw_text_length'] > 0]
train_df.shape

train_df.isna().sum()

sns.countplot(x='label', data=train_df, palette='Set3')
plt.title("Number of Fake and Genuine News after dropping missing values")
plt.show()

"""**I decided to start over again**. So, I would fill all nans with a blank and strip them next, then, remove the zero-length texts and that should be good to start the preprocessing."""

sns.boxplot(y='raw_text_length', x='label', data=train_df, palette="Set3")
plt.title("Boxplot of the length of characters in Fake and Genuine Articles")
plt.show()

train_df['raw_text_length'].describe()

train_df[train_df['raw_text_length']==10]

"""These look like comments in the dataset. It will be difficult to separate. So I will keep them as it is and work on the next steps.

It so appeared after that there are more texts that have single-digit lengths or as low as 10. They seemed more like comments than proper texts. I will keep them for the time being as it is and move on to the next step.

#Text Preprocessing
"""

len(train_df['author'].unique())

gen_news_authors = set(list(train_df[train_df['label']==0]['author'].unique()))
fake_news_authors = set(list(train_df[train_df['label']==1]['author'].unique()))

overlapped_authors = gen_news_authors.intersection(fake_news_authors)

len(gen_news_authors), len(fake_news_authors), len(overlapped_authors)

train_df.head()

"""So before I began with text preprocessing, I actually looked at the overlapping number of authors that have fake and genuine articles. In other words, would having the author’s information be helpful in any way? I found out that there are 3838 authors, out of which 2226 are genuine and 1976 are fake news’ authors. 5 authors among them are both genuine and fake news’ authors."""

original_train_df = train_df.copy()

"""#Text Cleaning


1. Remove special characters
2. Expand contractions
3. Convert to lower-case
4. Word Tokenize
5. Remove Stopwords










"""

def preprocess_text(x):
  cleaned_text = re.sub(r'[^a-zA-Z\d\s\']+', '', x)
  word_list = []
  for each_word in cleaned_text.split(' '):
    try:
      word_list.append(contractions.fix(each_word).lower())
    except:
      print(x)
  return " ".join(word_list)
  
text_cols = ['text', 'title', 'author']
for col in text_cols:
  print("Processing column: {}".format(col))
  train_df[col] = train_df[col].apply(lambda x: preprocess_text(x))
  test_df[col] = test_df[col].apply(lambda x: preprocess_text(x))

"""#Got Error because of some sort of Turkish/Slavic language

ABÇin ilişkilerinde ABD ve NATOnun etkisi yazan Manlio Dinucci Uluslararası bir forumda konuşan İtalyan coğrafyacı Manlio Dinucci ABDnin tüm dünyaya egemen olabilmek için sahip olduğu silahların analizini bireşimleştirdi Suriye Rusya ve Çinin bugün elde silah herkesin açıkça kabul ettiği bu üstünlüğü dünyanın bu tek kutuplu örgütlenişi tartışılır hale getirmesinden dolayı bu makale daha da önem kazanmaktadır

Therefore I rearranged the order of preprocessing
"""

text_cols = ['text', 'title', 'author']

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for col in text_cols:
#   print("Processing column: {}".format(col))
#   train_df[col] = train_df[col].apply(lambda x: preprocess_text(x))
#   test_df[col] = test_df[col].apply(lambda x: preprocess_text(x))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for col in text_cols:
#   print("Processing column: {}".format(col))
#   train_df[col] = train_df[col].apply(word_tokenize)
#   test_df[col] = test_df[col].apply(word_tokenize)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for col in text_cols:
#   print("Processing column: {}".format(col))
#   train_df[col] = train_df[col].apply(lambda x: [each_word for each_word in x if each_word not in stopwords])
#   test_df[col] = test_df[col].apply(lambda x: [each_word for each_word in x if each_word not in stopwords])

"""#Wordcloud"""

# since count vectorizer expects strings

train_df['text_joined'] = train_df['text'].apply(lambda x: " ".join(x))
test_df['text_joined'] = test_df['text'].apply(lambda x: " ".join(x))

train_df.head()

#iterate through the csv file
all_texts_gen = " ".join(train_df[train_df['label']==0]['text_joined'])
all_texts_fake = " ".join(train_df[train_df['label']==1]['text_joined'])

wordcloud = WordCloud(width = 900, height = 400,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(all_texts_gen)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.title("Genuine News")
plt.show()

wordcloud = WordCloud(width = 900, height = 400,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(all_texts_fake)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.title("Fake News")
plt.show()

"""In the fake news wordcloud, the frequency of some words is strikingly higher than the others. On the genuine news’ wordcloud, there is a mix of different font sizes. On the contrary, in the fake news dataset, the smaller texts are in the background and some of the words are used much more frequently. There are fewer medium-sized words in the fake news wordcloud or, in other words, there is a disconnect in progressively diminishing frequency of appearance. The frequency is either high or low.

#Stylometric Analysis

The stylometric analysis is often referred to as the analysis of the author’s style. I will look into a few of the stylometric features such as the number of sentences per article, the average words per sentence in an article, the average length of words per article, and the POS tag counts.
"""

original_train_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# original_train_df['sent_tokens'] = original_train_df['text'].apply(sent_tokenize)

original_train_df.head()

"""**Number of Sentences per Article**

To get this I needed the original dataset since I have lost the sentence information in train_df. So, I saved a copy of the actual data in orginal_train_df which I used to convert the sentences to sequences.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# original_train_df['len_sentence'] = original_train_df['sent_tokens'].apply(len)

original_train_df.head()

"""Now, I am looking at the count of the sentences by each target category as follows:"""

sns.boxplot(y='len_sentence', x='label', data=original_train_df, palette="Set3")
plt.title("Boxplot of Number of Sentences in Fake and Genuine Articles")
plt.show()

"""Evidently, fake articles have a lot of outliers but 75% of the fake articles have the number of sentences lower than the 50% of the genuine news articles.

**Average No. of Words per Sentence Article**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# original_train_df['sent_word_tokens'] = original_train_df['sent_tokens'].apply(lambda x: [word_tokenize(each_sentence) for each_sentence in x])

original_train_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def get_seq_tokens_cleaned(seq_tokens):
#   no_punc_seq = [each_seq.translate(str.maketrans('', '', string.punctuation)) for each_seq in seq_tokens]
#   sent_word_tokens = [word_tokenize(each_sentence) for each_sentence in no_punc_seq]
#   return sent_word_tokens
# 
# original_train_df['sent_word_tokens'] = original_train_df['sent_tokens'].apply(lambda x: get_seq_tokens_cleaned(x))

original_train_df.head()

def get_average_words_in_sent(seq_word_tokens):
  return np.mean([len(seq) for seq in seq_word_tokens])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# original_train_df['avg_words_per_sent'] = original_train_df['sent_word_tokens'].apply(lambda x: get_average_words_in_sent(x))

original_train_df['avg_words_per_sent'].describe()

sns.boxplot(y='avg_words_per_sent', x='label', data=original_train_df, palette="Set3")
plt.title("Boxplot of the Average Number of Words per Sentence in Fake and Genuine Articles")
plt.show()

"""Here, I counted the total number of words per sentence in each article and returned the average. Then I plotted the counts in a boxplot to visualize them. **It is seen that, on average, fake articles are wordier than genuine ones.**

**Average Word Length per Article**
"""

def get_average_word_length(seq_word_tokens):
  return np.mean([len(word) for seq in seq_word_tokens for word in seq])

original_train_df['avg_word_length'] = original_train_df['sent_word_tokens'].apply(lambda x: get_average_word_length(x))

sns.boxplot(y='avg_word_length', x='label', data=original_train_df, palette="Set3")
plt.title("Boxplot of the Average Length of Words per Article in Fake and Genuine Articles")
plt.show()

"""**This is the average word length in one article. In the box plot, it is evident that the average word length is higher in the fake articles.**

**POS Tag Counts**
"""

all_tokenized_gen = [a for b in train_df[train_df['label']==0]['text'].tolist() for a in b]
all_tokenized_fake = [a for b in train_df[train_df['label']==1]['text'].tolist() for a in b]

len(all_tokenized_gen), len(all_tokenized_fake)

def get_post_tags_list(tokenized_articles):
  all_pos_tags = []
  for word in tokenized_articles:
    pos_tag = nltk.pos_tag([word])[0][1]
    all_pos_tags.append(pos_tag)
  return all_pos_tags

# Commented out IPython magic to ensure Python compatibility.
# %%time
# all_pos_tagged_word_gen = get_post_tags_list(all_tokenized_gen)
# all_pos_tagged_word_fake = get_post_tags_list(all_tokenized_fake)

all_pos_tagged_word_gen[:5], all_pos_tagged_word_fake[:5]

gen_pos_df = pd.DataFrame(dict(Counter(all_pos_tagged_word_gen)).items(), columns=['Pos_tag', 'Genuine News'])
fake_pos_df = pd.DataFrame(dict(Counter(all_pos_tagged_word_fake)).items(), columns=['Pos_tag', 'Fake News'])

gen_pos_df

fake_pos_df

pos_df = gen_pos_df.merge(fake_pos_df, on='Pos_tag')

# Make percentage for comparison
pos_df['Genuine News'] = pos_df['Genuine News'] * 100 / pos_df['Genuine News'].sum()
pos_df['Fake News'] = pos_df['Fake News'] * 100 / pos_df['Fake News'].sum()
pos_df.head()

# plot a multiple bar chart
pos_df.plot.bar(width=0.7)
plt.xticks(range(0,len(pos_df['Pos_tag'])), pos_df['Pos_tag'])
plt.show()

"""I tried to look at the part-of-speech (POS) combinations in Fake vs Genuine articles. I only stored the POS of the words into a list while iterating through each article, put the respective POS count in one DataFrame, and used a bar plot to show the percentage combination of the POS tags in Fake and News articles. The Nouns are much higher in both the articles. In general, there is no distinct pattern except for the percentage of past-tense verbs in fake news is half of that in the genuine ones. Apart from that, all other POS types are almost equal in fake and genuine articles.

#Model Training

##Text Classification using Machine Learning
"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.model_selection import GridSearchCV

import warnings
warnings.filterwarnings('ignore')

train_df['text_joined'] = train_df['text'].apply(lambda x: " ".join(x))
test_df['text_joined'] = test_df['text'].apply(lambda x: " ".join(x))

target = train_df['label'].values

count_vectorizer = CountVectorizer(ngram_range=(1, 2))
tf_idf_transformer = TfidfTransformer(smooth_idf=False)

# fit train data to count vectorizer
count_vectorizer.fit(train_df['text_joined'].values)
count_vect_train = count_vectorizer.transform(train_df['text_joined'].values)

# fit ngrams count to tfidf transformers
tf_idf_transformer.fit(count_vect_train)
tf_idf_train = tf_idf_transformer.transform(count_vect_train)

# Transform the test data as well
count_vect_test = count_vectorizer.transform(test_df['text_joined'].values)
tf_idf_test = tf_idf_transformer.transform(count_vect_test)

"""##Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(tf_idf_train, target, random_state=0)

"""Once the analysis is complete, I took first the conventional way of using the Count Vectorizer and term frequency-inverse document frequency or Tf-idf. The Count Vectorizer, as configured in the code, generates bigrams as well. The counts of their occurrences are obtained in the form of a matrix using the CountVectorizer() and this word-count matrix is then transformed into the normalized term-frequency (tf-idf) representation. Here, I have used smooth=False, to avoid zero division error. By providing smooth=False, I am basically adding one to the document frequency since it is the denominator in the formula for idf calculation, as shown below —

**idf(t) = log [ n / (df(t) + 1) ]**
"""

df_perf_metrics = pd.DataFrame(columns=['Model', 'Accuracy_Training_Set', 'Accuracy_Test_Set', 'Precision', 'Recall', 'f1_score'])

"""#Machine Learning Classifier Training and Validating
**Benchmarking with Default Configurations**
"""

df_perf_metrics = pd.DataFrame(columns=[
    'Model', 'Accuracy_Training_Set', 'Accuracy_Test_Set', 'Precision',
    'Recall', 'f1_score', 'Training Time (secs'
])
models_trained_list = []


def get_perf_metrics(model, i):
    # model name
    model_name = type(model).__name__
    # time keeping
    start_time = time.time()
    print("Training {} model...".format(model_name))
    # Fitting of model
    model.fit(X_train, y_train)
    print("Completed {} model training.".format(model_name))
    elapsed_time = time.time() - start_time
    # Time Elapsed
    print("Time elapsed: {:.2f} s.".format(elapsed_time))
    # Predictions
    y_pred = model.predict(X_test)
    # Add to ith row of dataframe - metrics
    df_perf_metrics.loc[i] = [
        model_name,
        model.score(X_train, y_train),
        model.score(X_test, y_test),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred), "{:.2f}".format(elapsed_time)
    ]
    # keep a track of trained models
    models_trained_list.append(model)
    print("Completed {} model's performance assessment.".format(model_name))

""" I intended to train the models with the default configurations and pick out the best-performing model to tune later. For this, I looped through a list and saved all the performance metrics into another DataFrame and the models in a list.
 
"""

models_list = [LogisticRegression(),
               MultinomialNB(),
               RandomForestClassifier(),
               DecisionTreeClassifier(),
               GradientBoostingClassifier(),
               AdaBoostClassifier()]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for n, model in enumerate(models_list):
#     get_perf_metrics(model, n)

df_perf_metrics

"""###Adding Title and Author Information to the Text

To check if there is any improvement
"""

train_df['all_info'] = train_df['text'] + train_df['title'] + train_df['author']
train_df['all_info'] = train_df['all_info'].apply(lambda x: " ".join(x))

test_df['all_info'] = test_df['text'] + test_df['title'] + test_df['author']
test_df['all_info'] = test_df['all_info'].apply(lambda x: " ".join(x))

tf_idf_transformer = TfidfTransformer(smooth_idf=False)
count_vectorizer = CountVectorizer(ngram_range=(1, 2))
count_vect_train = count_vectorizer.fit_transform(train_df['all_info'].values)
tf_idf_train = tf_idf_transformer.fit_transform(count_vect_train)

X_train, X_test, y_train, y_test = train_test_split(tf_idf_train,
                                                    target,
                                                    random_state=0)

# Transform the test data
count_vect_test = count_vectorizer.transform(test_df['all_info'].values)
tf_idf_test = tf_idf_transformer.transform(count_vect_test)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for n, model in enumerate(models_list):
#     get_perf_metrics(model, n)

df_perf_metrics

"""I used Logistic Regression, Multinomial Naive Bayes, Decision Trees, Random Forest, Gradient Boost, and Ada Boost classifiers. The precision of MultinomialNB is the best among all, but f1-score falters because of the poor recall score. In fact, recall is the worst at 68%. The best models in the results were Logistic Regression and AdaBoost whose results are similar. I chose to go with Logistic Regression to save training time.

##Tuning the Logistic Regression Model

**GridSearchCV for Tuning Logistic Regression Classifier**
"""

model = LogisticRegression()

max_iter = [100, 200, 500, 1000]
C = [0.1, 0.5, 1, 10, 50, 100]

param_grid = dict(max_iter=max_iter, C=C)

grid = GridSearchCV(estimator=model,
                    param_grid=param_grid,
                    cv=5,
                    scoring=['f1'],
                    refit='f1',
                    verbose=2)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# grid_result = grid.fit(X_train, y_train)

grid_result.best_estimator_

grid_result.best_params_

model = grid_result.best_estimator_
y_pred = model.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred))
print('Precision: ', precision_score(y_test, y_pred))
print('Recall: ', recall_score(y_test, y_pred))
print('f1-score: ', f1_score(y_test, y_pred))

"""Tune my chosen classifier. I started out with a wider range for max_iter and C. Then used GirdSearchCV with cv=r, i.e. 5 folds for cross-validation since label distribution is fairly distributed. I have used f1-score for scoring and used refit to return the trained model with the best f1-score.

##Retuning the Logistic Regression Classifier

####Attempt 1

In the last grid, the lowest of the max_iter list (100) and highest of the C list (100) were the best values. In this attempt, I will see if there are better fits in the values beyond these, lower than 100 for max_iter or just a bit higher, and higher than 100 for C or just a bit lower.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = LogisticRegression()
# 
# max_iter = [50, 75, 100]
# C = [75, 100, 125]
# 
# param_grid = dict(max_iter=max_iter, C=C)
# 
# grid = GridSearchCV(estimator=model,
#                     param_grid=param_grid,
#                     cv=5,
#                     scoring=['f1'],
#                     refit='f1',
#                     verbose=2)
# 
# grid_result = grid.fit(X_train, y_train)

model = grid_result.best_estimator_
y_pred = model.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred))
print('Precision: ', precision_score(y_test, y_pred))
print('Recall: ', recall_score(y_test, y_pred))
print('f1-score: ', f1_score(y_test, y_pred))

grid_result.best_params_

"""####Attempt 2

In the previous attempt, we saw max_iter is still at 100 while C = 125 resulted into a marginally higher precision and consequently the f1 score improved just a bit. Let's see if we can improve the results a little more.

For this I have kept C constant and I have stated a range of C from 120 to 150 which increases by a step value of 10.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = LogisticRegression()
# 
# max_iter = [100]
# C = [120, 130, 140, 150]
# 
# param_grid = dict(max_iter=max_iter, C=C)
# 
# grid = GridSearchCV(estimator=model,
#                     param_grid=param_grid,
#                     cv=5,
#                     scoring=['f1'],
#                     refit='f1',
#                     verbose=2)
# 
# grid_result = grid.fit(X_train, y_train)

model = grid_result.best_estimator_
y_pred = model.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred))
print('Precision: ', precision_score(y_test, y_pred))
print('Recall: ', recall_score(y_test, y_pred))
print('f1-score: ', f1_score(y_test, y_pred))

grid_result.best_params_

"""###**Attempt 3**
One final attempt to find the best model, keeping C = 100 and max_iter = 100, 125, 140.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = LogisticRegression()
# 
# max_iter = [100]
# C = [100, 125, 140]
# 
# param_grid = dict(max_iter=max_iter, C=C)
# 
# grid = GridSearchCV(estimator=model,
#                     param_grid=param_grid,
#                     cv=5,
#                     scoring=['f1'],
#                     refit='f1',
#                     verbose=2)
# 
# grid_result = grid.fit(X_train, y_train)

model = grid_result.best_estimator_
y_pred = model.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred))
print('Precision: ', precision_score(y_test, y_pred))
print('Recall: ', recall_score(y_test, y_pred))
print('f1-score: ', f1_score(y_test, y_pred))

grid_result.best_params_

"""The best resulting model had an accuracy of 97.62% and an f1-score of 97.60%. For both, we have achieved 4% improvement. Now, I noticed that the max_iter’s best value was 100, which was the lower boundary of the range, and for C, it was also 100, but it was the upper boundary of the range. So, to accommodate parameter search, I used max_iter = 50, 70 , 100 and C = 75, 100, 125. There was a marginal improvement with max_iter=100 and C=125. So, I decided to keep that constant and scaled up the parameter search for C from 120 to 150, with step size 10. All performance metrics were equal for this run to the starting grid’s results. However, the value of C=140 for this run."""